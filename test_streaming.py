#!/usr/bin/env python3
"""
æµ‹è¯•OpenAIä»£ç†çš„æµå¼å“åº”åŠŸèƒ½
"""
import requests
import json
import time

# ä»£ç†é…ç½®
PROXY_URL = "http://127.0.0.1:8080"
WEB_URL = "http://127.0.0.1:8081"

def test_streaming_request():
    """æµ‹è¯•æµå¼è¯·æ±‚"""
    print("ğŸŒŠ æµ‹è¯•æµå¼å“åº”...")
    
    # æ¨¡æ‹ŸOpenAIæµå¼APIè¯·æ±‚
    test_data = {
        "model": "gpt-3.5-turbo",
        "messages": [
            {"role": "user", "content": "è¯·å†™ä¸€é¦–å…³äºç¼–ç¨‹çš„çŸ­è¯—"}
        ],
        "max_tokens": 150,
        "stream": True  # å¯ç”¨æµå¼å“åº”
    }
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer sk-test-key-for-demo"
    }
    
    try:
        print(f"ğŸ“¡ å‘é€æµå¼è¯·æ±‚åˆ°ä»£ç†æœåŠ¡å™¨: {PROXY_URL}/v1/chat/completions")
        
        # å‘é€æµå¼è¯·æ±‚åˆ°ä»£ç†
        response = requests.post(
            f"{PROXY_URL}/v1/chat/completions",
            headers=headers,
            json=test_data,
            stream=True,  # å¯ç”¨æµå¼æ¥æ”¶
            timeout=30
        )
        
        print(f"ğŸ“Š å“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“‹ å“åº”å¤´: {dict(response.headers)}")
        
        if response.status_code == 200:
            print("ğŸ“¥ æ¥æ”¶æµå¼æ•°æ®:")
            chunk_count = 0
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:
                    chunk_count += 1
                    print(f"  å— {chunk_count}: {len(chunk)} å­—èŠ‚")
                    # æ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
                    chunk_text = chunk.decode('utf-8', errors='ignore')
                    print(f"    å†…å®¹: {chunk_text[:100]}...")
                    
                    if chunk_count >= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªå—
                        print("    ...")
                        break
            
            print(f"âœ… æµå¼å“åº”æµ‹è¯•å®Œæˆï¼å…±æ¥æ”¶ {chunk_count} ä¸ªæ•°æ®å—")
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {response.text}")
        
        print(f"ğŸŒ è¯·åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹ç›‘æ§ç•Œé¢: {WEB_URL}")
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

def test_non_streaming_request():
    """æµ‹è¯•éæµå¼è¯·æ±‚"""
    print("\nğŸ“„ æµ‹è¯•éæµå¼å“åº”...")
    
    # æ¨¡æ‹ŸOpenAIéæµå¼APIè¯·æ±‚
    test_data = {
        "model": "gpt-3.5-turbo",
        "messages": [
            {"role": "user", "content": "Hello, world!"}
        ],
        "max_tokens": 50
        # æ³¨æ„ï¼šæ²¡æœ‰ stream: True
    }
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer sk-test-key-for-demo"
    }
    
    try:
        print(f"ğŸ“¡ å‘é€éæµå¼è¯·æ±‚åˆ°ä»£ç†æœåŠ¡å™¨: {PROXY_URL}/v1/chat/completions")
        
        # å‘é€éæµå¼è¯·æ±‚åˆ°ä»£ç†
        response = requests.post(
            f"{PROXY_URL}/v1/chat/completions",
            headers=headers,
            json=test_data,
            timeout=30
        )
        
        print(f"ğŸ“Š å“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“ å“åº”å†…å®¹: {response.text[:200]}...")
        
        print("âœ… éæµå¼å“åº”æµ‹è¯•å®Œæˆï¼")
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

def test_different_endpoints():
    """æµ‹è¯•ä¸åŒçš„APIç«¯ç‚¹"""
    print("\nğŸ”„ æµ‹è¯•ä¸åŒAPIç«¯ç‚¹...")
    
    endpoints = [
        ("/v1/models", "GET", None),
        ("/v1/completions", "POST", {
            "model": "text-davinci-003",
            "prompt": "Hello",
            "max_tokens": 10
        }),
        ("/v1/embeddings", "POST", {
            "model": "text-embedding-ada-002",
            "input": "Hello world"
        })
    ]
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer sk-test-key-for-demo"
    }
    
    for endpoint, method, data in endpoints:
        try:
            print(f"ğŸ“¡ æµ‹è¯• {method} {endpoint}")
            
            if method == "GET":
                response = requests.get(f"{PROXY_URL}{endpoint}", headers=headers, timeout=10)
            else:
                response = requests.post(f"{PROXY_URL}{endpoint}", headers=headers, json=data, timeout=10)
            
            print(f"   çŠ¶æ€ç : {response.status_code}")
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸŒŠ OpenAI ä»£ç†æµå¼å“åº”æµ‹è¯•å·¥å…·")
    print("=" * 60)
    
    # æµ‹è¯•æµå¼å“åº”
    test_streaming_request()
    
    # æµ‹è¯•éæµå¼å“åº”
    test_non_streaming_request()
    
    # æµ‹è¯•ä¸åŒç«¯ç‚¹
    test_different_endpoints()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   â€¢ ä»£ç†æœåŠ¡å™¨: {PROXY_URL}")
    print(f"   â€¢ ç›‘æ§ç•Œé¢: {WEB_URL}")
    print("   â€¢ æµå¼å’Œéæµå¼è¯·æ±‚éƒ½ä¼šè¢«æ­£ç¡®å¤„ç†å’Œè®°å½•")
    print("   â€¢ æ‰€æœ‰è¯·æ±‚è¯¦æƒ…å¯åœ¨Webç•Œé¢ä¸­æŸ¥çœ‹")
    print("=" * 60)
